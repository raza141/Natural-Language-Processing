{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Mining Process (NPL Level 1):\n",
    "\n",
    "#### Data Preprocessing & Data Normalization (Cleaning and Transforming)\n",
    "\n",
    "- **_Tokenization(Data Preprocessing):_**\n",
    "    - It split the paragraph into words and sentences. \n",
    "    - Word Tokenization `(nltk.word_tokenize(word))`\n",
    "    - Sentence Tokenization `(nltk.sent_tokenization(Paragraph))`\n",
    "- **_StopWord removal (Data Preprocessing):_**\n",
    "    - Its removing the `(i,me, we,that etc from nltk.corpus import stopwords)`\n",
    "    - `stopwords = nltk.corpus.stopwords.words('english')`\n",
    "    - stopwrods = [w for w in words if w not in stopwords]\n",
    "- **_Stemming (Data Normalization):_**\n",
    "    - It remove the suffixes from words to obtain the root word.\n",
    "    - `PorterStemmer() Algorithm`  Its very basic and widely used.\n",
    "    - `SnowballStemmer() Algorithm` Its extension of porterstemmer and avilable in multi language.\n",
    "    - `LancasterStemmer() Algorithm` Its very aggressive to convert words into root words.\n",
    "- **_Lemmatization (Data Normalization):_**\n",
    "    - Unlike to Stemming Lemmatization does not remove the suffixes, but converts the word into dictionary form with respect to context.\n",
    "    - Lemmatization results in real words that make linguistic sense.\n",
    "    - `WordNetLemmatizer()` is a popular algorithm for lemmatization.\n",
    "    - lemma = nltk.stem.WordNetLemmatizer()\n",
    "    - lemma.lemmatize(word)\n",
    "- **_POS Tagging_**\n",
    "    - `nltk.pos_tag(lammas)`\n",
    "    -  POS tagging is very cruicial and fundamental step in NPL, Sentiment may based on Adjective and Adverbs, and POS tagging helps us to determine the sentiment of the sentence. \n",
    "- **_BoW [Method]:_**\n",
    "    - Bag of Words is mehtod in which we convert the words into vectors, using the frequency of the word. Each word is represented by a vector and the vectors are combined to form a matrix. each word treated as a column in the matrix(Features) and treated independently.\n",
    "    - As each word is treated as a feature, it is called Bag of words, results its doesn't handle the order of the words in a sentence nor plural words or synonyms.\n",
    "    - `CountVectorizer()` from sklearn.feature_extraction.text import CountVectorizer`\n",
    "- **_TF-IDF [Method]:_**\n",
    "    - TF-IDF is a measure of how important a word is to a document in a collection of documents.\n",
    "    - as good is frequently used in three documents which mean its not useful, hence got lower TF-IDF score. \n",
    "    TF-IDF is kind of an extension of BoW method.\n",
    "    - `TfidfVectorizer()` from sklearn.feature_extraction.text import TfidfVectorizer`\n",
    "    - TfidfVectorizer required input in list of string not list of lists, mean after lemmatization you have to join them back.\n",
    "- **_Word2vec_**:\n",
    "    - Word2vec is word embedding technique in NLP tasks.\n",
    "    - Word2Vec assigns each word in a given corpus a high-dimensional vector representation. This representation is learned by training a neural network on a large text dataset. The idea is that words with similar meanings will have similar vector.\n",
    "    - Word2vec is available in `gensim` library \n",
    "    - from `gensim.model` import `word2vec` \n",
    "    - `model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)`\n",
    "    - sg: Skip-gram model (1) or Continuous Bag of Words (0).\n",
    "    - vector_size: The dimensionality of the Word2Vec word vectors. (Commom value is 100 - 300 smalller size when we have lower dataset)\n",
    "    - window: The maximum distance between the current and predicted word within a sentence. (range between 2 - 5)\n",
    "    - min_count: Ignores all words with a total frequency lower than this.\n",
    "    - sentences should be in list of list not a flat list (e.g. {'I', 'love', 'Python'})\n",
    "\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.968609865470852\n",
      "Your email is Spam! Hope you don't get it\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import sklearn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the pipeline globally\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesses the text by removing special characters, converting to lowercase,\n",
    "    and lemmatizing the words.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to be preprocessed.\n",
    "    \n",
    "    Returns:\n",
    "        str: The preprocessed text.\n",
    "    \"\"\"\n",
    "    lemma = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    msgs = re.sub('[^a-zA-Z]', ' ', text).lower().split()  # removing special characters and converting to lowercase\n",
    "    msgs = [lemma.lemmatize(word) for word in msgs if word not in stop_words]  # lemmatizing the words and removing stopwords\n",
    "    msgs = ' '.join(msgs)  # joining the words again to form a sentence \n",
    "    \n",
    "    return msgs\n",
    "\n",
    "def train_and_evaluate_model(df: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Trains and evaluates a logistic regression model on the given dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe containing the dataset.\n",
    "    \n",
    "    Returns:\n",
    "        float: The accuracy score of the model.\n",
    "    \"\"\"\n",
    "    df['Spam'] = pd.get_dummies(df['Category'], drop_first=True)  # encoding the target variable\n",
    "    y = df['Spam']  # target variable\n",
    "    messages = df['Message']  # feature / input variable\n",
    "    corpus = [preprocess_text(i) for i in messages]  # preprocessing the input text\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(corpus, y, test_size=0.2, random_state=42)  # splitting the dataset into train and test\n",
    "    pipeline.fit(x_train, y_train)\n",
    "    pred = pipeline.predict(x_test)\n",
    "    acc_score = accuracy_score(y_test, pred)\n",
    "    \n",
    "    return acc_score\n",
    "\n",
    "# Reading the dataset:\n",
    "df = pd.read_csv('spam.csv')\n",
    "\n",
    "# Train and evaluate the model:\n",
    "accuracy = train_and_evaluate_model(df)\n",
    "print(f\"Accuracy Score: {accuracy}\")\n",
    "\n",
    "# Reading the User email as input:    \n",
    "user_input = input(\"Enter your email: \")\n",
    "user_email = preprocess_text(user_input)\n",
    "prediction = pipeline.predict([user_email])\n",
    "if prediction[0] == 0:\n",
    "    print(\"Your email is Not Spam\")\n",
    "else:\n",
    "    print(\"Your email is Spam! Hope you don't get it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Using text embeding technique (Word2vec) - ML Model Dicesion Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SentimentAna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
